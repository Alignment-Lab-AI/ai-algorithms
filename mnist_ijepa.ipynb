{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9911b5c52d4a808ee59379cb7e984b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3752 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a4db19f75b44f5a69ee179cf0a50a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class SimplifiedViT(nn.Module):\n",
    "    \"\"\"Simplified Vision Transformer (ViT) implementation\"\"\"\n",
    "    def __init__(self, image_size, patch_size, dim, depth, heads, mlp_dim, in_channels):\n",
    "        super(SimplifiedViT, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = in_channels * patch_size * patch_size\n",
    "\n",
    "        # Embed patches to vectors\n",
    "        self.patch_embedding = nn.Linear(patch_dim, dim)\n",
    "        # Learnable position embeddings\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, dim))\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim))\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Handle different input shapes\n",
    "        if img.dim() == 3 and img.size(1) == self.num_patches:\n",
    "            x = img\n",
    "        else:\n",
    "            if img.dim() == 3:\n",
    "                img = img.unsqueeze(1)\n",
    "            \n",
    "            b, c, h, w = img.shape\n",
    "            assert h == w == self.image_size, f\"Input image size ({h}*{w}) doesn't match model ({self.image_size}*{self.image_size}).\"\n",
    "            assert c == self.in_channels, f\"Input channels {c} doesn't match expected {self.in_channels}\"\n",
    "\n",
    "            # Convert image to patches\n",
    "            x = img.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "            x = x.contiguous().view(b, c, -1, self.patch_size * self.patch_size)\n",
    "            x = x.permute(0, 2, 1, 3).contiguous().view(b, -1, c * self.patch_size * self.patch_size)\n",
    "            x = self.patch_embedding(x)\n",
    "\n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        # Pass through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Apply final normalization\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class IJEPA(nn.Module):\n",
    "    \"\"\"Simplified IJEPA (Image-based Joint-Embedding Predictive Architecture)\"\"\"\n",
    "    def __init__(self, image_size, patch_size, dim, context_depth, predictor_depth, heads, mlp_dim, in_channels):\n",
    "        super(IJEPA, self).__init__()\n",
    "        # Context and target encoders (sharing architecture but not weights)\n",
    "        self.context_encoder = SimplifiedViT(image_size, patch_size, dim, context_depth, heads, mlp_dim, in_channels)\n",
    "        self.target_encoder = SimplifiedViT(image_size, patch_size, dim, context_depth, heads, mlp_dim, in_channels)\n",
    "\n",
    "        # Predictor network\n",
    "        predictor_layers = []\n",
    "        for _ in range(predictor_depth):\n",
    "            predictor_layers.extend([\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(dim)\n",
    "            ])\n",
    "        self.predictor = nn.Sequential(*predictor_layers)\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # Decoder for visualizing predictions\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, in_channels * patch_size * patch_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, context, target):\n",
    "        # Encode context and target\n",
    "        context_features = self.context_encoder(context)\n",
    "        with torch.no_grad():\n",
    "            target_features = self.target_encoder(target)\n",
    "        \n",
    "        # Predict target features from context\n",
    "        predicted_features = self.predictor(context_features)\n",
    "        \n",
    "        return predicted_features, target_features\n",
    "\n",
    "    def predict_target_features(self, features):\n",
    "        # Decode features to image patches\n",
    "        patches = self.decoder(features)\n",
    "        \n",
    "        b, n, _ = patches.shape\n",
    "        patches = patches.view(b, n, self.in_channels, self.patch_size, self.patch_size)\n",
    "        patches = patches.permute(0, 2, 1, 3, 4)\n",
    "        \n",
    "        # predict image from patches\n",
    "        images = patches.contiguous().view(b, self.in_channels, self.image_size, self.image_size)\n",
    "        return images\n",
    "\n",
    "def generate_masks(image, patch_size, mask_size_ratio=0.5, border_width=1):\n",
    "    \"\"\"Randomly generate square-shaped masks for context and target\"\"\"\n",
    "    b, c, h, w = image.shape\n",
    "    num_patches = h // patch_size\n",
    "    mask_size = max(1, int(num_patches * mask_size_ratio))\n",
    "    \n",
    "    patches = image.view(b, c, num_patches, patch_size, num_patches, patch_size)\n",
    "    patch_means = patches.mean(dim=(1, 3, 5))\n",
    "    \n",
    "    masks = []\n",
    "    for i in range(b):\n",
    "        # Find the patch with maximum intensity\n",
    "        max_patch = torch.argmax(patch_means[i].view(-1))\n",
    "        max_y, max_x = max_patch // num_patches, max_patch % num_patches\n",
    "        \n",
    "        # Center the mask around the maximum intensity patch\n",
    "        start_y = max(0, min(num_patches - mask_size, max_y - mask_size // 2))\n",
    "        start_x = max(0, min(num_patches - mask_size, max_x - mask_size // 2))\n",
    "        \n",
    "        mask = torch.zeros(h, w, dtype=torch.float, device=image.device)\n",
    "        \n",
    "        # Square-shaped mask\n",
    "        mask[start_y*patch_size:(start_y+mask_size)*patch_size, \n",
    "             start_x*patch_size:(start_x+mask_size)*patch_size] = 1\n",
    "        \n",
    "        # Add blue border\n",
    "        mask[start_y*patch_size:(start_y+mask_size)*patch_size, \n",
    "             start_x*patch_size:start_x*patch_size+border_width] = 2  # Left border\n",
    "        mask[start_y*patch_size:(start_y+mask_size)*patch_size, \n",
    "             (start_x+mask_size)*patch_size-border_width:(start_x+mask_size)*patch_size] = 2  # Right border\n",
    "        mask[start_y*patch_size:start_y*patch_size+border_width, \n",
    "             start_x*patch_size:(start_x+mask_size)*patch_size] = 2  # Top border\n",
    "        mask[(start_y+mask_size)*patch_size-border_width:(start_y+mask_size)*patch_size, \n",
    "             start_x*patch_size:(start_x+mask_size)*patch_size] = 2  # Bottom border\n",
    "        \n",
    "        masks.append(mask)\n",
    "    \n",
    "    return torch.stack(masks).unsqueeze(1)\n",
    "\n",
    "def train(model, dataloader, optimizer, device, num_epochs):\n",
    "    \"\"\"Train and live preview logic\"\"\"\n",
    "    model.train()\n",
    "    criterion_features = nn.MSELoss(reduction='sum')\n",
    "    criterion_reconstruction = nn.MSELoss(reduction='sum')\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    losses, feature_losses, reconstruction_losses, times, learning_rates = [], [], [], [], []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_batches = len(dataloader) * num_epochs\n",
    "    \n",
    "    progress_bar = tqdm(total=total_batches, desc=\"Training\", unit=\"batch\")\n",
    "    out = widgets.Output()\n",
    "    display(out)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    plt.close(fig)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = epoch_feature_loss = epoch_reconstruction_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Generate masks for this batch\n",
    "            mask = generate_masks(data, model.patch_size)\n",
    "            \n",
    "            context_mask = (mask == 0).float()\n",
    "            target_mask = (mask == 1).float()\n",
    "            border_mask = (mask > 1).float()\n",
    "            \n",
    "            # Create context and target\n",
    "            context = data * context_mask + 0.8 * target_mask + torch.tensor([0.0, 0.0, 1.0]).view(1, 3, 1, 1).to(device) * border_mask\n",
    "            target = data * target_mask + 0.8 * context_mask + torch.tensor([0.0, 0.0, 1.0]).view(1, 3, 1, 1).to(device) * border_mask\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_features, target_features = model(context, target)\n",
    "            \n",
    "            # Prepare masks for loss calculation\n",
    "            target_mask_patches = target_mask.view(target_mask.size(0), target_mask.size(1), -1, model.patch_size, model.patch_size).any(dim=(3, 4)).float()\n",
    "            target_mask_patches = target_mask_patches.view(target_mask_patches.size(0), -1, 1)\n",
    "            \n",
    "            # Calculate losses\n",
    "            pred_features_masked = pred_features * target_mask_patches\n",
    "            target_features_masked = target_features * target_mask_patches\n",
    "            \n",
    "            loss_features = criterion_features(pred_features_masked, target_features_masked) / (target_mask_patches.sum() * pred_features.size(-1) + 1e-6)\n",
    "            \n",
    "            pred_image = model.predict_target_features(pred_features)\n",
    "            loss_reconstruction = criterion_reconstruction(pred_image * target_mask, data * target_mask) / (target_mask.sum() + 1e-6)\n",
    "            \n",
    "            loss = loss_features + 0.1 * loss_reconstruction\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running losses\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_feature_loss += loss_features.item()\n",
    "            epoch_reconstruction_loss += loss_reconstruction.item()\n",
    "            \n",
    "            # Update target encoder with momentum\n",
    "            momentum = 0.996 + (1.0 - 0.996) * (epoch / num_epochs)\n",
    "            for param, target_param in zip(model.context_encoder.parameters(), model.target_encoder.parameters()):\n",
    "                target_param.data = momentum * target_param.data + (1 - momentum) * param.data\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\"Epoch\": epoch+1, \"Loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            # Visualize (live preview)\n",
    "            if batch_idx % 2 == 0:\n",
    "                with out:\n",
    "                    out.clear_output(wait=True)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        combined_image = context * context_mask + pred_image * target_mask + torch.tensor([0.0, 0.0, 1.0]).view(1, 3, 1, 1).to(device) * border_mask\n",
    "                    \n",
    "                    for i, (title, img) in enumerate([('Original', data.repeat(1, 3, 1, 1)), ('Context (Masked)', context), ('Target (Masked)', target), ('Predicted', combined_image)]):\n",
    "                        axs[i].clear()\n",
    "                        axs[i].imshow(img[0].permute(1, 2, 0).detach().cpu(), vmin=0, vmax=1)\n",
    "                        axs[i].set_title(title)\n",
    "                        axs[i].axis('off')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    display(fig)\n",
    "        \n",
    "        # Compute epoch averages and update lists\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        avg_feature_loss = epoch_feature_loss / len(dataloader)\n",
    "        avg_reconstruction_loss = epoch_reconstruction_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        feature_losses.append(avg_feature_loss)\n",
    "        reconstruction_losses.append(avg_reconstruction_loss)\n",
    "        times.append(time.time() - start_time)\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        progress_bar.set_postfix({\"Epoch\": epoch+1, \"Avg Loss\": f\"{avg_loss:.4f}\"})\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return losses, feature_losses, reconstruction_losses, times, learning_rates\n",
    "\n",
    "def plots(losses, feature_losses, reconstruction_losses, times, learning_rates):\n",
    "    \"\"\"Plot training metrics\"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 15))\n",
    "    \n",
    "    # Loss vs Epoch\n",
    "    axs[0, 0].plot(range(1, len(losses) + 1), losses, label='Total Loss')\n",
    "    axs[0, 0].plot(range(1, len(feature_losses) + 1), feature_losses, label='Feature Loss')\n",
    "    axs[0, 0].plot(range(1, len(reconstruction_losses) + 1), reconstruction_losses, label='Reconstruction Loss')\n",
    "    axs[0, 0].set_xlabel('Epoch')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].set_title('Loss vs Epoch')\n",
    "    axs[0, 0].legend()\n",
    "    \n",
    "    # Loss vs Time\n",
    "    axs[0, 1].plot(times, losses, label='Total Loss')\n",
    "    axs[0, 1].plot(times, feature_losses, label='Feature Loss')\n",
    "    axs[0, 1].plot(times, reconstruction_losses, label='Reconstruction Loss')\n",
    "    axs[0, 1].set_xlabel('Time (seconds)')\n",
    "    axs[0, 1].set_ylabel('Loss')\n",
    "    axs[0, 1].set_title('Loss vs Training Time')\n",
    "    axs[0, 1].legend()\n",
    "    \n",
    "    # Learning Rate vs Epoch\n",
    "    axs[1, 0].plot(range(1, len(learning_rates) + 1), learning_rates)\n",
    "    axs[1, 0].set_xlabel('Epoch')\n",
    "    axs[1, 0].set_ylabel('Learning Rate')\n",
    "    axs[1, 0].set_title('Learning Rate vs Epoch')\n",
    "    \n",
    "    # Loss Ratio vs Epoch\n",
    "    loss_ratios = [r / f for r, f in zip(reconstruction_losses, feature_losses)]\n",
    "    axs[1, 1].plot(range(1, len(loss_ratios) + 1), loss_ratios)\n",
    "    axs[1, 1].set_xlabel('Epoch')\n",
    "    axs[1, 1].set_ylabel('Reconstruction Loss / Feature Loss')\n",
    "    axs[1, 1].set_title('Loss Ratio vs Epoch')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Transform data to tensors\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Load MNIST dataset\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = IJEPA(image_size=28, patch_size=4, dim=256, context_depth=6, predictor_depth=6, heads=8, mlp_dim=512, in_channels=3).to(device)\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        list(model.context_encoder.parameters()) + \n",
    "        list(model.predictor.parameters()) + \n",
    "        list(model.decoder.parameters()), \n",
    "        lr=1e-3, \n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    num_epochs = 4\n",
    "    losses, feature_losses, reconstruction_losses, times, learning_rates = train(model, train_loader, optimizer, device, num_epochs)\n",
    "    \n",
    "    plots(losses, feature_losses, reconstruction_losses, times, learning_rates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
